[
  {
    "objectID": "speakers.html",
    "href": "speakers.html",
    "title": "Speakers",
    "section": "",
    "text": "Speaker lineup: Coming Soon.\nIf you would like to recommend a speaker, contact: TBD."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Schedule: Coming Soon.\nDates: Nov 12–13, 2025 Location: 100% Online Talks will be recorded Speakers participate in live Q&A Time zone: US Eastern Time\n\nDay 1: TBD\nDay 2: TBD"
  },
  {
    "objectID": "venue.html",
    "href": "venue.html",
    "title": "Venue",
    "section": "",
    "text": "Venue details: Online.\n\nLocation: Online event\nTime zone: US Eastern Time (ET)\nTravel and lodging: TBD\nAccessibility: TBD"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Risk 2026",
    "section": "",
    "text": "R!sk 2026 is an R Consortium event focused on risk analytics with R. The program includes a two day schedule of talks and lightning talks."
  },
  {
    "objectID": "about.html#goals",
    "href": "about.html#goals",
    "title": "About Risk 2026",
    "section": "Goals",
    "text": "Goals\n\nShowcase practical risk analytics with R across industries.\nShare advances in risk modeling, measurement, and reporting.\nConnect the community through hands-on learning."
  },
  {
    "objectID": "about.html#quick-facts",
    "href": "about.html#quick-facts",
    "title": "About Risk 2026",
    "section": "Quick Facts",
    "text": "Quick Facts\nDates: February 18–19, 2026\nLocation: 100% online\nRecordings: Talks will be recorded\nCommunity: Speakers participate in live Q&A\nTime Zone: Schedule is US Eastern Time Zone\nPricing: Industry, Academia, and Student level ticketing are all available"
  },
  {
    "objectID": "about.html#registration-pricing",
    "href": "about.html#registration-pricing",
    "title": "About Risk 2026",
    "section": "Registration Pricing",
    "text": "Registration Pricing\nThe following table lists the registration fees for three categories of attendees: students, academics and members of non-profit organizations, and professionals employed in industry who do not have non-profit status. All prices are in U.S. dollars.\n\n\n\nCategory\nPrice\n\n\n\n\nStudents\n$25\n\n\nAcademic / Non-profit\n$50\n\n\nIndustry\n$70"
  },
  {
    "objectID": "about.html#register-now",
    "href": "about.html#register-now",
    "title": "About Risk 2026",
    "section": "Register Now!",
    "text": "Register Now!"
  },
  {
    "objectID": "cfp.html",
    "href": "cfp.html",
    "title": "Call for Proposals",
    "section": "",
    "text": "The R Consortium invites submissions for our inaugural online R!sk event, a global gathering for anyone using the R statistical programming language to calculate, measure, report, and mitigate risk.\nWe welcome talks about risk in all areas where it’s an impactful element in your work—from healthcare and climate to cybersecurity, supply chain, and beyond. We’re looking for presentations that showcase how R helps you succeed in managing, analyzing, and communicating risk across diverse domains.\nR!sk 2026 brings together practitioners, researchers, and industry experts to share innovative tools, methods, and case studies that advance the science and practice of risk analysis in R."
  },
  {
    "objectID": "cfp.html#call-for-proposals-is-now-closed.",
    "href": "cfp.html#call-for-proposals-is-now-closed.",
    "title": "Call for Proposals",
    "section": "",
    "text": "The R Consortium invites submissions for our inaugural online R!sk event, a global gathering for anyone using the R statistical programming language to calculate, measure, report, and mitigate risk.\nWe welcome talks about risk in all areas where it’s an impactful element in your work—from healthcare and climate to cybersecurity, supply chain, and beyond. We’re looking for presentations that showcase how R helps you succeed in managing, analyzing, and communicating risk across diverse domains.\nR!sk 2026 brings together practitioners, researchers, and industry experts to share innovative tools, methods, and case studies that advance the science and practice of risk analysis in R."
  },
  {
    "objectID": "cfp.html#important-dates",
    "href": "cfp.html#important-dates",
    "title": "Call for Proposals",
    "section": "Important Dates",
    "text": "Important Dates\nCall for Proposals Opens: November 7, 2025\nProposal Submission Deadline: December 14, 2025\nNotification of Acceptance: December 30, 2025\nEvent Dates: February 18-19, 2026"
  },
  {
    "objectID": "cfp.html#suggested-topic-areas",
    "href": "cfp.html#suggested-topic-areas",
    "title": "Call for Proposals",
    "section": "Suggested Topic Areas",
    "text": "Suggested Topic Areas\nWe welcome proposals from all disciplines where risk is calculated, managed, or communicated using R. Submissions may cover, but are not limited to, the following categories:\n\nCore Risk Modeling and Quantification\n\nProbabilistic and stochastic risk modeling\nBayesian methods for risk estimation\nMonte Carlo simulation and uncertainty quantification\nSensitivity analysis and model validation\nExtreme value theory and tail-risk modeling\nScenario analysis and stress testing\nSpatial and temporal risk modeling\nQuantitative risk scoring systems in R\n\n\n\nMeasurement, Data, and Reporting\n\nData pipelines for risk assessment in R (tidyverse, data.table, etc.)\nReproducible risk reporting (R Markdown, Quarto, Shiny)\nRisk dashboards and visual analytics (Shiny, ggplot2, Plotly, etc.)\nCommunicating uncertainty and probabilistic outcomes\nRisk metric standardization and validation frameworks\nReproducibility and regulatory transparency in risk reporting\n\n\n\nDomain-Specific Applications\n\nFinancial and credit risk modeling (VaR, expected shortfall, etc.)\nInsurance and actuarial risk modeling\nEnvironmental and climate risk assessment\nEpidemiological and health risk analysis\nSupply chain and operational risk management\nCybersecurity and information risk modeling\nEnergy market and grid reliability risk assessment\n\n\n\nTools, Frameworks, and Infrastructure\n\nR packages for risk modeling (e.g., {riskr}, {VaR}, {actuar}, {EnvStats})\nIntegration of R with Python, C++, or cloud systems for risk computation\nR in production risk workflows (Docker, Plumber APIs, Shiny Server)\nAutomated model validation and testing in R\nHigh-performance computing and parallelization for large risk simulations\nReproducibility, version control, and data lineage in risk analytics\n\n\n\nGovernance, Mitigation, and Decision-Making\n\nRisk appetite, thresholds, and tolerance modeling\nGovernance frameworks and compliance (Basel III, Solvency II, etc.)\nEthical risk modeling and algorithmic transparency\nDecision science and risk-informed decision-making\nMitigation strategies and optimization under uncertainty\nHuman-in-the-loop risk modeling and interpretability\n\n\n\nEmerging Topics and Innovations\n\nAI and machine learning approaches to risk prediction\nCausal inference and counterfactual risk analysis\nGraph and network-based risk models\nAgent-based modeling and simulation of systemic risk\nReinforcement learning and adaptive risk systems\nGenerative models and synthetic data for risk analysis"
  },
  {
    "objectID": "cfp.html#submission-types",
    "href": "cfp.html#submission-types",
    "title": "Call for Proposals",
    "section": "Submission Types",
    "text": "Submission Types\nWe encourage a variety of proposal types to foster an inclusive and diverse program. Possible submission types include:\n\nTalks (20–30 minutes): Technical presentations, case studies, or research findings\nLightning Talks (5–10 minutes): Short, focused ideas or tools\nPanels (45–60 minutes): Discussions among experts or practitioners\nTutorials/Workshops (60–90 minutes): Hands-on sessions demonstrating tools or methodologies"
  },
  {
    "objectID": "cfp.html#submission-guidelines",
    "href": "cfp.html#submission-guidelines",
    "title": "Call for Proposals",
    "section": "Submission Guidelines",
    "text": "Submission Guidelines\nAll proposals should include the following information:\n\nTitle of the presentation\nAuthor(s) and affiliation(s)\nAbstract (250–400 words)\nSession type (Talk, Lightning Talk, Panel, Tutorial)\nBrief bio of the presenter(s)\nAny links to relevant materials (e.g., GitHub, published papers, Shiny apps)"
  },
  {
    "objectID": "cfp.html#review-criteria",
    "href": "cfp.html#review-criteria",
    "title": "Call for Proposals",
    "section": "Review Criteria",
    "text": "Review Criteria\n\nRelevance to risk analysis and the R community\nClarity and quality of abstract\nPractical impact and innovation\nReproducibility and transparency of results\nDiversity of perspectives and applications"
  },
  {
    "objectID": "cfp.html#contact",
    "href": "cfp.html#contact",
    "title": "Call for Proposals",
    "section": "Contact",
    "text": "Contact\nFor questions regarding this Call for Proposals or the Risk! Digital Event, please contact:\nThe Risk! Conference Team\ninfo@r-consortium.org\nLearn more about the R Consortium and its initiatives at https://r-consortium.org."
  },
  {
    "objectID": "attend.html",
    "href": "attend.html",
    "title": "Risk 2026",
    "section": "",
    "text": "The R Consortium and its working groups are dedicated to providing a harassment-free experience for participants at all of our events, whether they are held in person or virtually. R Consortium events are working conferences intended for professional networking and collaboration within the open source community. They exist to encourage the open exchange of ideas and expression and require an environment that recognizes the inherent worth of every person and group. While at R Consortium events or related ancillary or social events, any participants, including members, speakers, attendees, volunteers, sponsors, exhibitors, booth staff and anyone else, should not engage in harassment in any form.\nThis Code of Conduct may be revised at any time by The R Consortium and the terms are non-negotiable. Your registration for or attendance at any R Consortium event, whether it’s held in person or virtually, indicates your agreement to abide by this policy and its terms.\n\n\nAll event participants, whether they are attending an in-person event or a virtual event, are expected to behave in accordance with professional standards, with both this Code of Conduct as well as their respective employer’s policies governing appropriate workplace behavior and applicable laws.\n\n\n\nHarassment will not be tolerated in any form, whether in person or virtually, including, but not limited to, harassment based on sex, gender, sexual orientation, disability, physical appearance, body size, race, age, religion or any other status protected by laws in which the conference or program is being held. Harassment includes the use of abusive, offensive or degrading language, intimidation, stalking, harassing photography or recording, inappropriate physical contact, sexual imagery and unwelcome sexual advances or requests for sexual favors. Any report of harassment at one of our events, whether in person or virtual, will be addressed immediately. Participants asked to stop any harassing behavior are expected to comply immediately. Anyone who witnesses or is subjected to unacceptable behavior should notify a conference organizer at once.\nExhibitors should not use sexualized images, activities, or other material in their booths and must refrain from the use of sexualized clothing, uniforms, costumes, or otherwise creating a sexualized environment. Speakers should not use sexual language, images, or any language or images that would constitute harassment as defined above in their talks.\nIndividuals who participate (or plan to participate) in R Consortium events, whether its an in-person event or a virtual event, should conduct themselves at all times in a manner that comports with both the letter and spirit of this policy prohibiting harassment and abusive behavior, whether before, during or after the event. This includes statements made in social media postings, on-line publications, text messages, and all other forms of electronic communication.\n\n\n\nIf a participant engages in harassing behavior, whether in person or virtually, the conference organizers may take any action they deem appropriate depending on the circumstances, ranging from issuance of a warning to the offending individual to expulsion from the conference with no refund. The R Consortium reserves the right to exclude any participant found to be engaging in harassing behavior from participating in any further R Consortium events, working groups, trainings or other activities.\nIf a participant (or individual wishing to participate in an R Consortium event, in-person and/or virtual), through postings on social media or other online publications or another form of electronic communication, engages in conduct that violates this policy, whether before, during or after a R Consortium event, the R Consortium may take appropriate corrective action, which could include imposing a temporary or permanent ban on an individual’s participation in future R Consortium events, events, working groups, trainings or other activities.\n\n\n\nIf you are being harassed, notice that someone else is being harassed, or have any other concerns relating to harassment, please contact a member of the conference staff immediately. You are also encouraged to contact conduct@r-consortium.org.\n\n\n\nOur staff has taken incident response training and responds to harassment reports quickly and thoroughly. As referenced above, if a participant engages in harassing behavior, whether in-person or virtually, the conference organizers may take any action they deem appropriate, ranging from issuance of a warning to the offending individual to expulsion from the conference with no refund, depending on the circumstances. The R Consortium reserves the right to exclude any participant found to be engaging in harassing behavior from participating in any further R Consortium events, working groups, trainings or other activities.\nConference staff will also provide support to victims, including, but not limited to:\n\nProviding an Escort\nContacting Hotel/Venue Security or Local Law Enforcement\nBriefing Key Event Staff For Response/Victim Assistance\nAnd otherwise assisting those experiencing harassment to ensure that they feel safe for the duration of the conference.\n\n\n\n\nIf you are planning to attend an upcoming event, whether in-person or virtually and have concerns regarding another individual who may be present, please contact conduct@r-consortium.org. Precautions will be taken to ensure your comfort and safety, including, but not limited to providing an escort, prepping onsite event staff, keeping victim and harasser from attending the same talks/social events and providing onsite contact cell phone numbers for immediate contact."
  },
  {
    "objectID": "attend.html#code-of-conduct",
    "href": "attend.html#code-of-conduct",
    "title": "Risk 2026",
    "section": "",
    "text": "The R Consortium and its working groups are dedicated to providing a harassment-free experience for participants at all of our events, whether they are held in person or virtually. R Consortium events are working conferences intended for professional networking and collaboration within the open source community. They exist to encourage the open exchange of ideas and expression and require an environment that recognizes the inherent worth of every person and group. While at R Consortium events or related ancillary or social events, any participants, including members, speakers, attendees, volunteers, sponsors, exhibitors, booth staff and anyone else, should not engage in harassment in any form.\nThis Code of Conduct may be revised at any time by The R Consortium and the terms are non-negotiable. Your registration for or attendance at any R Consortium event, whether it’s held in person or virtually, indicates your agreement to abide by this policy and its terms.\n\n\nAll event participants, whether they are attending an in-person event or a virtual event, are expected to behave in accordance with professional standards, with both this Code of Conduct as well as their respective employer’s policies governing appropriate workplace behavior and applicable laws.\n\n\n\nHarassment will not be tolerated in any form, whether in person or virtually, including, but not limited to, harassment based on sex, gender, sexual orientation, disability, physical appearance, body size, race, age, religion or any other status protected by laws in which the conference or program is being held. Harassment includes the use of abusive, offensive or degrading language, intimidation, stalking, harassing photography or recording, inappropriate physical contact, sexual imagery and unwelcome sexual advances or requests for sexual favors. Any report of harassment at one of our events, whether in person or virtual, will be addressed immediately. Participants asked to stop any harassing behavior are expected to comply immediately. Anyone who witnesses or is subjected to unacceptable behavior should notify a conference organizer at once.\nExhibitors should not use sexualized images, activities, or other material in their booths and must refrain from the use of sexualized clothing, uniforms, costumes, or otherwise creating a sexualized environment. Speakers should not use sexual language, images, or any language or images that would constitute harassment as defined above in their talks.\nIndividuals who participate (or plan to participate) in R Consortium events, whether its an in-person event or a virtual event, should conduct themselves at all times in a manner that comports with both the letter and spirit of this policy prohibiting harassment and abusive behavior, whether before, during or after the event. This includes statements made in social media postings, on-line publications, text messages, and all other forms of electronic communication.\n\n\n\nIf a participant engages in harassing behavior, whether in person or virtually, the conference organizers may take any action they deem appropriate depending on the circumstances, ranging from issuance of a warning to the offending individual to expulsion from the conference with no refund. The R Consortium reserves the right to exclude any participant found to be engaging in harassing behavior from participating in any further R Consortium events, working groups, trainings or other activities.\nIf a participant (or individual wishing to participate in an R Consortium event, in-person and/or virtual), through postings on social media or other online publications or another form of electronic communication, engages in conduct that violates this policy, whether before, during or after a R Consortium event, the R Consortium may take appropriate corrective action, which could include imposing a temporary or permanent ban on an individual’s participation in future R Consortium events, events, working groups, trainings or other activities.\n\n\n\nIf you are being harassed, notice that someone else is being harassed, or have any other concerns relating to harassment, please contact a member of the conference staff immediately. You are also encouraged to contact conduct@r-consortium.org.\n\n\n\nOur staff has taken incident response training and responds to harassment reports quickly and thoroughly. As referenced above, if a participant engages in harassing behavior, whether in-person or virtually, the conference organizers may take any action they deem appropriate, ranging from issuance of a warning to the offending individual to expulsion from the conference with no refund, depending on the circumstances. The R Consortium reserves the right to exclude any participant found to be engaging in harassing behavior from participating in any further R Consortium events, working groups, trainings or other activities.\nConference staff will also provide support to victims, including, but not limited to:\n\nProviding an Escort\nContacting Hotel/Venue Security or Local Law Enforcement\nBriefing Key Event Staff For Response/Victim Assistance\nAnd otherwise assisting those experiencing harassment to ensure that they feel safe for the duration of the conference.\n\n\n\n\nIf you are planning to attend an upcoming event, whether in-person or virtually and have concerns regarding another individual who may be present, please contact conduct@r-consortium.org. Precautions will be taken to ensure your comfort and safety, including, but not limited to providing an escort, prepping onsite event staff, keeping victim and harasser from attending the same talks/social events and providing onsite contact cell phone numbers for immediate contact."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Risk 2026",
    "section": "",
    "text": "Join us at R!sk 2026, our conference dedicated to the open-source R community and risk analytics."
  },
  {
    "objectID": "index.html#register-now",
    "href": "index.html#register-now",
    "title": "Risk 2026",
    "section": "Register Now!",
    "text": "Register Now!\n\nDates: February 18–19, 2026\n\n\nLocation: 100% online\nWhether you’re calculating risk in finance, measuring risk in healthcare, reporting risk in insurance, or mitigating risk across any industry—there’s something here for you.\nExperience inspiring talks and hands-on workshops, all designed to empower R users at every level.\n\nTalks will be recorded\n\nSpeakers participate in live Q&A\n\nSchedule is US Eastern Time Zone"
  },
  {
    "objectID": "index.html#keynote-speaker",
    "href": "index.html#keynote-speaker",
    "title": "Risk 2026",
    "section": "Keynote speaker",
    "text": "Keynote speaker\n\nKeynote: James “JD” Long (CTO at Palomar)\nThe R Consortium is pleased to announce that James “JD” Long, CTO at Palomar, and co-author of R Cookbook: Proven Recipes for Data Analysis, Statistics, and Graphics, will deliver the keynote address at R!sk 2026.\n\n\nAbout JD Long\n\nJD Long is CTO at Palomar. He says “I’m the guy who can build a Monte Carlo model, help parallelize the model to run on cloud services and then stand in front of a group of business leaders and put the work in context where everyone understands. My super power is thinking probabilistically, understanding risk, and communicating clearly.”\nJD is widely known in the R community for his practical focus on building real-world risk and insurance models and for helping large organizations adopt R and modern analytics practices. He is a frequent speaker at R conferences and meetups, and his work has been featured in the R Journal and other publications.\nAs co-author of R Cookbook: Proven Recipes for Data Analysis, Statistics, and Graphics, JD has helped thousands of R users solve everyday data and modeling problems. Through his talks and writing, he emphasizes probabilistic thinking, clear communication of uncertainty, and building analytics cultures that support experimentation and learning."
  },
  {
    "objectID": "index.html#registration-pricing",
    "href": "index.html#registration-pricing",
    "title": "Risk 2026",
    "section": "Registration Pricing",
    "text": "Registration Pricing\nThe following table lists the registration fees for three categories of attendees: students, academics and members of non-profit organizations, and professionals employed in industry who do not have non-profit status. All prices are in U.S. dollars.\n\n\n\nCategory\nPrice\n\n\n\n\nStudents\n$25\n\n\nAcademic / Non-profit\n$50\n\n\nIndustry\n$70"
  },
  {
    "objectID": "index.html#register-now-1",
    "href": "index.html#register-now-1",
    "title": "Risk 2026",
    "section": "Register Now!",
    "text": "Register Now!"
  },
  {
    "objectID": "index.html#sponsorships",
    "href": "index.html#sponsorships",
    "title": "Risk 2026",
    "section": "Sponsorships",
    "text": "Sponsorships\nIf you would like to know about sponsorship opportunities, please write to sponsorship@r-consortium.org"
  },
  {
    "objectID": "program.html",
    "href": "program.html",
    "title": "Risk 2026 Program",
    "section": "",
    "text": "All times Eastern\n\n\n\nStart Time\nEnd Time\nDuration\nTitle of Talk\nSpeaker and Title (affiliation)\n\n\n\n\n8:00 AM\n9:00 AM\n60 mins\nKeynote: When Everyone Can Code, What’s a Quant Still For?\nJD Long, CTO at Palomar, co-author of R Cookbook: Proven Recipes for Data Analysis, Statistics, and Graphics\n\n\n9:10 AM\n9:50 AM\n40 mins\nFully Reproducible Model Validation Reports Using Docker, R, Quarto, {renv} and {targets}\nMichael Thomas, Chief Data Scientist, Ketchbrook Analytics\n\n\n9:55 AM\n10:35 AM\n40 mins\nThe Distributions Tailored to Skewed and Fat Tails in the FatTailsR Package\nPatrice Kiener, Président - CEO, Inmodelia\n\n\n10:40 AM\n11:20 AM\n40 mins\nFinancial Dependence via ISE100 via vine copulas: R packages for risk modeling\nÖ. Ozan Evkaya, Lecturer in Statistics, School of Mathematics, University of Edinburgh; part-time remote lecturer, Istinye University\n\n\n11:25 AM\n12:05 PM\n40 mins\nBreak\n\n\n\n12:10 PM\n12:50 PM\n40 mins\nIdentifying Critical Participants in the SIC Payment System with R\nJulius Mattern, Senior data scientist, Swiss National BankChristoph Meyer, Senior data scientist, Swiss National Bank\n\n\n12:55 PM\n1:35 PM\n40 mins\nAgentic R Workflows for High-Stakes Risk Analysis\nGreg Michaelson, Co-founder and Chief Product Officer, Zerve\n\n\n1:40 PM\n2:20 PM\n40 mins\nHow the R packages NumericEnsembles and LogisticEnsembles can return the best possible results for data sets related to risk\nRuss Conte, Independent consultant\n\n\n2:25 PM\n3:05 PM\n40 mins\nA Bayesian R Framework for Quantifying Cyber Risk Using the FAIR Model and MITRE ATT&CK\nJoshua Connors, Cyber Security Risk Management Engineer, Viasat"
  },
  {
    "objectID": "program.html#day-1-wednesday-february-18-2026",
    "href": "program.html#day-1-wednesday-february-18-2026",
    "title": "Risk 2026 Program",
    "section": "",
    "text": "All times Eastern\n\n\n\nStart Time\nEnd Time\nDuration\nTitle of Talk\nSpeaker and Title (affiliation)\n\n\n\n\n8:00 AM\n9:00 AM\n60 mins\nKeynote: When Everyone Can Code, What’s a Quant Still For?\nJD Long, CTO at Palomar, co-author of R Cookbook: Proven Recipes for Data Analysis, Statistics, and Graphics\n\n\n9:10 AM\n9:50 AM\n40 mins\nFully Reproducible Model Validation Reports Using Docker, R, Quarto, {renv} and {targets}\nMichael Thomas, Chief Data Scientist, Ketchbrook Analytics\n\n\n9:55 AM\n10:35 AM\n40 mins\nThe Distributions Tailored to Skewed and Fat Tails in the FatTailsR Package\nPatrice Kiener, Président - CEO, Inmodelia\n\n\n10:40 AM\n11:20 AM\n40 mins\nFinancial Dependence via ISE100 via vine copulas: R packages for risk modeling\nÖ. Ozan Evkaya, Lecturer in Statistics, School of Mathematics, University of Edinburgh; part-time remote lecturer, Istinye University\n\n\n11:25 AM\n12:05 PM\n40 mins\nBreak\n\n\n\n12:10 PM\n12:50 PM\n40 mins\nIdentifying Critical Participants in the SIC Payment System with R\nJulius Mattern, Senior data scientist, Swiss National BankChristoph Meyer, Senior data scientist, Swiss National Bank\n\n\n12:55 PM\n1:35 PM\n40 mins\nAgentic R Workflows for High-Stakes Risk Analysis\nGreg Michaelson, Co-founder and Chief Product Officer, Zerve\n\n\n1:40 PM\n2:20 PM\n40 mins\nHow the R packages NumericEnsembles and LogisticEnsembles can return the best possible results for data sets related to risk\nRuss Conte, Independent consultant\n\n\n2:25 PM\n3:05 PM\n40 mins\nA Bayesian R Framework for Quantifying Cyber Risk Using the FAIR Model and MITRE ATT&CK\nJoshua Connors, Cyber Security Risk Management Engineer, Viasat"
  },
  {
    "objectID": "program.html#day-2-thursday-february-19-2026",
    "href": "program.html#day-2-thursday-february-19-2026",
    "title": "Risk 2026 Program",
    "section": "Day 2: Thursday, February 19, 2026",
    "text": "Day 2: Thursday, February 19, 2026\nAll times Eastern\n\n\n\nStart Time\nEnd Time\nDuration\nTitle of Talk\nSpeaker and Title (affiliation)\n\n\n\n\n8:00 AM\n8:15 AM\n15 mins\nDefining Granular Risk Groups: A Reproducible Workflow for Multi-Threshold Survival Analysis\nPayton Yau, Lecturer in Computational Biology, Nottingham Trent University (UK)\n\n\n8:15 AM\n8:30 AM\n15 mins\nRisky viz\nCara Thompson, Data visualisation consultant\n\n\n8:30 AM\n8:45 AM\n15 mins\nConformal Inference & Calibrating Uncertainty with a Three-Way Split\nFrank Hull, Director of data science & analytics\n\n\n8:45 AM\n9:00 AM\n15 mins\nLightweight Transfer Learning for Financial Forecasting Using Quasi-Randomized Networks\nThierry Moudiki, Founder and lead, Techtonique LLC\n\n\n9:00 AM\n9:15 AM\n15 mins\nShiny Apps for Time Series Analysis and Extremes in Hydrology\nAlonso Arriagada M., Senior Hydrologist Engineer, AUSENCO\n\n\n9:15 AM\n9:30 AM\n15 mins\nFrom Forecasts to Action: A Data Pipeline for Air Quality Monitoring and Notifications\nBjörn Bos, Environmental and behavioral economist, University of Hamburg\n\n\n9:30 AM\n10:10 AM\n40 mins\nDWD Temperature Explorer: An Open-Access R Shiny Dashboard for Climate-Informed Risk Governance in Germany\nDr. Cesar Ivan Alvarez, Research Scientist, Chair of Climate Resilience of Cultural Ecosystems, Center for Climate Resilience, University of Augsburg, Germany\n\n\n10:15 AM\n10:55 AM\n40 mins\nmcmodule: An R Package for Multi-Pathway Monte Carlo Risk Assessment\nNatalia Ciria, Veterinarian and PhD candidate in Epidemiology, Department of Animal Health and Anatomy, Autonomous University of Barcelona (UAB)\n\n\n11:05 AM\n11:45 AM\n40 mins\nMapping the Unknown During an Active Outbreak: Lessons from Geospatial Risk Modelling of Oropouche Virus\nAnna Frühauf, MD/PhD Candidate, Virus Epidemiology, Charité - Universitätsmedizin Berlin\n\n\n11:55 AM\n12:15 PM\n20 mins\nFinal Remarks\nTerry Christiani, Executive Director, R Consortium"
  },
  {
    "objectID": "registration.html",
    "href": "registration.html",
    "title": "Registration",
    "section": "",
    "text": "Registration information: Coming Soon.\n\nPricing tiers: TBD\nDiscounts: TBD\nRefund policy: TBD"
  },
  {
    "objectID": "workshops.html",
    "href": "workshops.html",
    "title": "R!sk 2026 Workshops",
    "section": "",
    "text": "Example Content"
  },
  {
    "objectID": "workshops.html#from-prompts-to-agents-building-evaluating-r-llm-workflows",
    "href": "workshops.html#from-prompts-to-agents-building-evaluating-r-llm-workflows",
    "title": "R!sk 2026 Workshops",
    "section": "From Prompts to Agents: Building & Evaluating R + LLM Workflows",
    "text": "From Prompts to Agents: Building & Evaluating R + LLM Workflows\n\n\n\nDescription\nThis hands-on workshop is a fast-track for R users who are new to generative AI. We’ll start with reliable prompting patterns in R, then add more context, tool calls, and agentic orchestration to automate multi-step tasks. Along the way we’ll cover tokens & context windows, prompt engineering best practices, model differences, safety basics, testing/QA, and lightweight testing/QC suitable for regulated pharma environments. Demos will feature packages such as ellmer and btw, alongside a small set of generic utilities for HTTP calls, validation, and logging (final package choices may vary). All code and slides will be open-sourced. Participants leave with runnable R scripts that covers a simple agentic workflow plus an evaluation pipeline they can adapt to their teams.\n\n\nTarget Audience & Prerequisites\nR analysts/scientists/engineers comfortable with data frames and basic scripting. No prior ML/AI required.\n\n\nDate & Time\nThursday, November 13, 2025\n1:20 PM - 4:20 PM Eastern (3 hours)\n\n\n\n\n\nDevin Pastoor, PhD\nChief Technology and Product Officer, A2-AI\n\nAs CTPO of A2-Ai, Devin works with clients to deliver end-to-end solutions that support quantitative decision-making. Devin specializes in bridging the unique characteristics of each client’s IT structure with the needs of the scientific team. Some of his focuses for clients include providing strategies to migrate from existing tools to more modern cloud-based workflows, and building prototype tools and dashboards to solve business challenges. Before joining A2-Ai, Devin worked as a principal solutions engineer to help dozens of organizations ranging from the FDA to small biotech and device manufacturers to develop and implement\n\n\nXu Fei\nSenior Solutions Engineer, A2-AI\n\nXu Fei is a Senior Solutions Engineer at A2-Ai, where he builds AI-powered tools and infrastructure for pharmaceutical research workflows. Working across R and Python stacks, he has developed LLM-enabled applications ranging from interactive chatbots to MCP server implementations, with a focus on making GenAI accessible and practical for scientific computing teams. His work bridges enterprise DevOps, cloud APIs (AWS Bedrock), and domain-specific R packages to help scientists integrate AI capabilities into their existing workflows.\n\n\nAathira Anil Kumar\nSoftware Engineer, A2-AI\n\nAathira Anil Kumar is an AI Engineer at A2-AI and a recent graduate of the University of Georgia, where she earned her Master’s in Artificial Intelligence. With over three years of experience researching and building with AI, she has worked with financial and semiconductor data and now focuses on developing generative AI tools for the pharma world—bridging R and Python ecosystems through open-source innovation and applied research."
  },
  {
    "objectID": "Abstracts.html",
    "href": "Abstracts.html",
    "title": "R!sk 2026 Abstracts",
    "section": "",
    "text": "CTO at Palomar, co-author of R Cookbook: Proven Recipes for Data Analysis, Statistics, and Graphics\n\n\n\nDate & Time: February 18 at 8:00am, Eastern\n\n\nQuant risk is in a weird new place: code is no longer the bottleneck. When LLMs make it easy for basically anyone to produce working R or Python code, the question stops being “can we build it?” and becomes “can we trust it, explain it, and reproduce it?”\nIn this keynote I’ll argue that what matters most now is system architecture, validation, and ownership, and all three get harder (and more important) when building gets cheap. Architecture isn’t just “tech design,” it’s policy: it nudges humans and LLMs to write code that’s modular, reviewable, and easy for someone else to verify. Validation has to level up too. LLM-assisted workflows are great at producing something that looks right very quickly.  However, testing can’t be the cleanup step at the end. We need strong automated tests, input/data checks, and backtesting that’s built into the way work ships.\nBut it’s not only about tests. It’s also about reproducibility: pinned environments, versioned data, and the ability to rerun a result and get the same answer—because if you can’t reproduce it, you can’t audit it, and you can’t own it. And as more people can push code, governance has to scale: clear code ownership, review gates, protected pipelines, and sensible guardrails that keep velocity high without turning production into a free-for-all. This is where the research/production line matters: experiments should be easy and encouraged; production should be boring, controlled, and observable.\nFinally, we need a culture shift. The computer isn’t responsible for your results. The IDE didn’t do it. The model didn’t do it. We did. In an “everyone can code” world, excellence looks less like personal wizardry and more like systems thinking: designing workflows where the right thing is the easy thing, and where accountability is clear when things go wrong. The goal isn’t to slow down. The goal is to make speed safe.\n\n\n\nJD Long is CTO at Palomar, He says “I’m the guy who can build a Monte Carlo model, help parallelize the model to run on cloud services and then stand in front of a group of business leaders and put the work in context where everyone understands. My super power is thinking probabilistically, understanding risk, and communicating clearly.”\nJD is widely known in the R community for his practical focus on building real-world risk and insurance models and for helping large organizations adopt R and modern analytics practices. He is a frequent speaker at R conferences and meetups, and his work has been featured in the R Journal and other publications.\nAs co-author of R Cookbook: Proven Recipes for Data Analysis, Statistics, and Graphics, JD has helped thousands of R users solve everyday data and modeling problems. Through his talks and writing, he emphasizes probabilistic thinking, clear communication of uncertainty, and building analytics cultures that support experimentation and learning."
  },
  {
    "objectID": "Abstracts.html#keynote-when-everyone-can-code-whats-a-quant-still-for",
    "href": "Abstracts.html#keynote-when-everyone-can-code-whats-a-quant-still-for",
    "title": "R!sk 2026 Abstracts",
    "section": "",
    "text": "Date & Time: February 18 at 8:00am, Eastern\n\n\nQuant risk is in a weird new place: code is no longer the bottleneck. When LLMs make it easy for basically anyone to produce working R or Python code, the question stops being “can we build it?” and becomes “can we trust it, explain it, and reproduce it?”\nIn this keynote I’ll argue that what matters most now is system architecture, validation, and ownership, and all three get harder (and more important) when building gets cheap. Architecture isn’t just “tech design,” it’s policy: it nudges humans and LLMs to write code that’s modular, reviewable, and easy for someone else to verify. Validation has to level up too. LLM-assisted workflows are great at producing something that looks right very quickly.  However, testing can’t be the cleanup step at the end. We need strong automated tests, input/data checks, and backtesting that’s built into the way work ships.\nBut it’s not only about tests. It’s also about reproducibility: pinned environments, versioned data, and the ability to rerun a result and get the same answer—because if you can’t reproduce it, you can’t audit it, and you can’t own it. And as more people can push code, governance has to scale: clear code ownership, review gates, protected pipelines, and sensible guardrails that keep velocity high without turning production into a free-for-all. This is where the research/production line matters: experiments should be easy and encouraged; production should be boring, controlled, and observable.\nFinally, we need a culture shift. The computer isn’t responsible for your results. The IDE didn’t do it. The model didn’t do it. We did. In an “everyone can code” world, excellence looks less like personal wizardry and more like systems thinking: designing workflows where the right thing is the easy thing, and where accountability is clear when things go wrong. The goal isn’t to slow down. The goal is to make speed safe.\n\n\n\nJD Long is CTO at Palomar, He says “I’m the guy who can build a Monte Carlo model, help parallelize the model to run on cloud services and then stand in front of a group of business leaders and put the work in context where everyone understands. My super power is thinking probabilistically, understanding risk, and communicating clearly.”\nJD is widely known in the R community for his practical focus on building real-world risk and insurance models and for helping large organizations adopt R and modern analytics practices. He is a frequent speaker at R conferences and meetups, and his work has been featured in the R Journal and other publications.\nAs co-author of R Cookbook: Proven Recipes for Data Analysis, Statistics, and Graphics, JD has helped thousands of R users solve everyday data and modeling problems. Through his talks and writing, he emphasizes probabilistic thinking, clear communication of uncertainty, and building analytics cultures that support experimentation and learning."
  },
  {
    "objectID": "Abstracts.html#fully-reproducible-model-validation-reports-using-docker-r-quarto-renv-and-targets",
    "href": "Abstracts.html#fully-reproducible-model-validation-reports-using-docker-r-quarto-renv-and-targets",
    "title": "R!sk 2026 Abstracts",
    "section": "Fully Reproducible Model Validation Reports Using Docker, R, Quarto, {renv} and {targets}",
    "text": "Fully Reproducible Model Validation Reports Using Docker, R, Quarto, {renv} and {targets}\nDate & Time: February 18 at 9:10am, Eastern\n\nAbstract\nAs Chief Data Scientist at Ketchbrook Analytics, it is Michael Thomas’s responsibility to ensure the robustness and reproducibility of every deliverable provided to the firm’s clients; this includes model validation reports that the firm authors. Over the past two years, Ketchbrook has built out an open source software architecture that provides full reproducibility in model validation reporting via Docker, R, Quarto, and the {renv} & {targets} R packages. This talk will demonstrate a workflow for creating model validation reports that are fully reproducible across different computing environments, ensuring regulatory compliance and scientific rigor in a quantitative risk management setting.\n\n\nBio\nMichael Thomas is the Chief Data Scientist at Ketchbrook Analytics, an international data science consulting firm. He holds a Bachelor’s Degree in Accounting and Mathematics from Stonehill College, and a Master’s Degree in Business Intelligence & Analytics from Saint Joseph’s University. He specializes in credit risk modeling and is passionate about bringing strong software development practices to his work. He is highly versed in the R, Python, and SQL programming languages, and holds a certification from RStudio (now Posit) in Data Science Education.\n\n\nSocial Media\n\nLinkedIn: https://www.linkedin.com/in/michaeljthomas2/\nBlueSky: @mike-thomas.bsky.social\nWebsite: https://www.ketchbrookanalytics.com"
  },
  {
    "objectID": "Abstracts.html#the-distributions-tailored-to-skewed-and-fat-tails-in-the-fattailsr-package",
    "href": "Abstracts.html#the-distributions-tailored-to-skewed-and-fat-tails-in-the-fattailsr-package",
    "title": "R!sk 2026 Abstracts",
    "section": "The Distributions Tailored to Skewed and Fat Tails in the FatTailsR Package",
    "text": "The Distributions Tailored to Skewed and Fat Tails in the FatTailsR Package\nDate & Time: February 18 at 9:55am, Eastern\n\nAbstract\nThe FatTailsR package [1], available on CRAN since 2014 and listed in the Distributions task view [2], introduces a new family of distributions on top of the logistic distribution built with 3 or 4 parameters for left and right fat tails. We give the design pattern of these distributions:\n\nThe version with 3 parameters (mu, gamma, kappa) is for symmetrical distributions and has closed forms for the pdf, cdf, quantile function and Radon-Nikodym derivate. Mu is the median, gamma is a scale parameter related to the density at the median, kappa is the shape and tail parameter. Kappa is exactly the Pareto exponent which means the various moments fall exactly at defined values of kappa: the kurtosis falls when kappa &lt;= 4, the variance when kappa &lt;= 2, the mean when kappa &lt;= 1. It is possible to build distributions with kappa &lt;= 1 and tails much thicker than the Cauchy distribution. When kappa tends to Infinity, the distribution tends to the 2-parameter logistic distribution.\nSeveral representations with 4 parameters are possible to describe skewed distributions. In addition to mu and gamma, we can use alpha and omega as the Pareto exponents of the left and right fat tails. We can also use kappa and delta where delta is the distortion of the distribution and by design its absolute value is always lower than kappa. We can also use kappa and epsilon where epsilon = delta/kappa is an eccentricity between -1 and +1 and is convenient for parameter estimation.\n\nFor all these 3 and 4 parameters versions, the quantile function, the Radon-Nikodym derivative, the moments, the Value-at-Risk and the Expected Shortfall have explicit forms unlike stable distributions.\n\nDesigning a stochastic process is easy. It is possible to design processes with a zero mean, a non-null median and a tail thicker than the other one.\nSeveral approaches are possible for the parameter estimation. A very interesting feature is that both the central part and the tails are estimated in one step, and accurately. The non-parametric approach is blazing fast. One parametric approach uses the logit of the empirical distribution.\nFitting such distributions, whether by MLE or any other techniques, provides a reliable estimate of the tail index on both sides, comparable to, or even better than the Hill estimator.\nWe have been using extensively these distributions in finance to estimate the distributions of asset returns on various periods, and use the corresponding VaR and ES to assess the underlying financial risk and to build optimized portfolios.\nExamples and videos over rolling windows picked in [3] will be presented during the talk.\n\n\n\nBio\nPatrice Kiener is the founder of InModelia, a consulting company in data science located in Paris (France) and active since 2010. He previously works in the food industry and from 2000 to 2009 at NETRAL, a software vendor of neural network software. Patrice maintains 3 R packages: RWsearch, FatTails and NNbenchmark. He is also the co-maintainer of the Distribution task view."
  },
  {
    "objectID": "Abstracts.html#financial-dependence-via-ise100-via-vine-copulas-r-packages-for-risk-modeling",
    "href": "Abstracts.html#financial-dependence-via-ise100-via-vine-copulas-r-packages-for-risk-modeling",
    "title": "R!sk 2026 Abstracts",
    "section": "Financial Dependence via ISE100 via vine copulas: R packages for risk modeling",
    "text": "Financial Dependence via ISE100 via vine copulas: R packages for risk modeling\nDate & Time: February 18 at 10:40am, Eastern\n\nAbstract\nThis study aims to investigate a Regular-Vine copula approach to estimate the interdependence structure of the Istanbul Stock Exchange index (ISE100). For this purpose, we consider 32 stocks related to 6 sectors belonging to ISE100.\nTo reflect the time-varying impacts of the 2008–2009 global financial crisis, the dependence analysis is conducted over pre-, during-, and post-global financial crisis periods. Portfolio analysis is considered via a rolling window approach to capture the changes in the dependence.\nWe compare the Regular-Vine-based generalized autoregressive conditional heteroskedasticity (GARCH) against the conventional GARCH model with different innovations. Value at risk and expected shortfall risk measures are used to validate the models.\nAdditionally, for the constructed portfolios, return performance is summarized using both Sharpe and Sortino ratios. To test the ability of the considered Regular-Vine approach on ISE100, another evaluation has been done during the COVID-19 pandemic crisis with various parameter settings.\nWhile presenting the case study results, some R programming details will be highlighted.\n\n\nBio\nI am a Lecturer in Statistics at the School of Mathematics, at the University of Edinburgh, and a part-time remote lecturer at Istinye University. In November 2023, I got the fellowship accreditation of Higher Education Academics in UK (FHEA). Previously, I held two postdoc positions at Padova University (2021) and KU Leuven (2020), after completing my PhD at Middle East Technical University in 2018.\nOutside of university teaching, I was a co-organiser of Technology Enhanced Mathematical Sciences Education (TEMSE) seminars in School of Math for 2023-2025, nowadays having the roles of Academic Cohort Lead (ACL), Generative AI TEMSE co-lead, EdinbR community and RSS Edinburgh local group member. In addition to my official teaching and research duties, I am ambitious about improving my statistical learning skills by leading/organizing/being a part of training workshops. I’m looking to collaborate on copulas and its applications, statistical methods for insurance and environment. Recently, I am interested in Gen-AI tools and their impacts on teaching and learning."
  },
  {
    "objectID": "Abstracts.html#identifying-critical-participants-in-the-sic-payment-system-with-r",
    "href": "Abstracts.html#identifying-critical-participants-in-the-sic-payment-system-with-r",
    "title": "R!sk 2026 Abstracts",
    "section": "Identifying Critical Participants in the SIC Payment System with R",
    "text": "Identifying Critical Participants in the SIC Payment System with R\nDate & Time: February 18 at 12:10pm, Eastern\n\nAbstract\nAs digital and instant payments spread, the resilience of central payment systems is increasingly important. We adapt a network‑and‑clustering framework to identify participants whose failure could disrupt system continuity, adding to the standard framework of Glowka et al. (2025) three practical dimensions — payment type (interbank vs customer), intrayear frequency, and transaction view (value vs volume) — and weighting scenario outcomes by economic activity to prioritise operational relevance. Applied to SIC (Switzerland’s central payment system), the method highlights critical roles played by mid‑sized domestic banks and, at times, financial‑market infrastructures alongside the consistently critical large international banks; volume‑based views shift prominence for some participants. Results are robust and useful for realistic stress tests and regulatory assessment.\nThe full reproducible workflow is implemented in R and will be shown step‑by‑step: data ingestion and wrangling (tidyverse, DBI, odbc, dbplyr), network and clustering analysis with parallelisation (tidygraph, tidymodels, tidyclust, furrr), and communication (ggplot2, patchwork, gt). Attendees will see code patterns, performance tips and visualisation techniques they can apply to their transaction or network datasets — no prior payments expertise required.\n\n\nBios\nJulius Mattern is a senior data scientist at the Swiss National Bank. He holds a master’s degree in political science and works with data from payment systems that process millions of transactions daily, with a focus on payment system risk analysis. He enjoys applying R to uncover insights that support informed decision-making.\nChristoph Meyer is a senior data scientist at the Swiss National Bank with a PhD in international macroeconomics and finance and a master’s in data science, specialising in payment-systems analysis and formerly focused on financial markets and business cycles; an enthusiastic R user for 10+ years."
  },
  {
    "objectID": "Abstracts.html#agentic-r-workflows-for-high-stakes-risk-analysis",
    "href": "Abstracts.html#agentic-r-workflows-for-high-stakes-risk-analysis",
    "title": "R!sk 2026 Abstracts",
    "section": "Agentic R Workflows for High-Stakes Risk Analysis",
    "text": "Agentic R Workflows for High-Stakes Risk Analysis\nDate & Time: February 18 at 12:55pm, Eastern\n\nAbstract\nAgentic R coding enables autonomous workflows that help analysts build, test, and refine risk models while keeping every step transparent and reproducible. This talk shows how R agents can construct end-to-end risk analysis pipelines, explore uncertainty through simulation and stress testing, and generate interpretable outputs tied directly to executable R code. Rather than replacing analysts, agentic workflows accelerate iteration, surface hidden assumptions, and improve model robustness. Attendees will learn practical patterns for using agentic R coding responsibly in high-stakes risk analysis.\n\n\nBio\nGreg Michaelson is a product leader, entrepreneur, and data scientist focused on building tools that help people do real work with data. He is the co-founder and Chief Product Officer of Zerve, where he designs agent-centric workflows that bridge analytics, engineering, and AI. Greg has led teams across product, data science, and infrastructure, with experience spanning startups, applied research, and large-scale analytics systems. He is known for translating complex technical ideas into practical products, and for building communities through hackathons, education, and content. Greg previously worked on forecasting and modeling efforts during the pandemic and continues to advocate for thoughtful, human-centered approaches to data and AI."
  },
  {
    "objectID": "Abstracts.html#how-the-r-packages-numericensembles-and-logisticensembles-can-return-the-best-possible-results-for-data-sets-related-to-risk",
    "href": "Abstracts.html#how-the-r-packages-numericensembles-and-logisticensembles-can-return-the-best-possible-results-for-data-sets-related-to-risk",
    "title": "R!sk 2026 Abstracts",
    "section": "How the R packages NumericEnsembles and LogisticEnsembles can return the best possible results for data sets related to risk",
    "text": "How the R packages NumericEnsembles and LogisticEnsembles can return the best possible results for data sets related to risk\nDate & Time: February 18 at 1:40pm, Eastern\n\nAbstract\nRuss developed four R packages that use AI and ensembles to automatically give results that consistently beat previous results for a given data set. His next chapter in life will be sharing these results with the world. These packages are his answer to a simple question: If you could build your dream data science solution, what would it be? You’ll see his answer in his talk.\n\n\nBio\nRuss Conte has done a wide range of things in life. A few (of many) include earning the #1 spot at his university in an annual mathematics competition (twice), earning a mathematics degree at the same time as a music degree, working as a professional counselor for ten years, working in (and sometime managing) multi-million dollar businesses for a multi-billion dollar global company, President of the Chicago Apple User Group, musician on a wide range of instruments, and a local leader for Amnesty International."
  },
  {
    "objectID": "Abstracts.html#a-bayesian-r-framework-for-quantifying-cyber-risk-using-the-fair-model-and-mitre-attck",
    "href": "Abstracts.html#a-bayesian-r-framework-for-quantifying-cyber-risk-using-the-fair-model-and-mitre-attck",
    "title": "R!sk 2026 Abstracts",
    "section": "A Bayesian R Framework for Quantifying Cyber Risk Using the FAIR Model and MITRE ATT&CK",
    "text": "A Bayesian R Framework for Quantifying Cyber Risk Using the FAIR Model and MITRE ATT&CK\nDate & Time: February 18 at 2:25pm, Eastern\n\nAbstract\nQuantifying cyber risk remains a challenge for information security teams due to sparse incident data, rapidly evolving attacker behaviors, and the difficulty of integrating technical security controls with financial loss modeling. This talk presents a fully open, R-based implementation of a quantitative risk model that combines the Factor Analysis of Information Risk (FAIR) taxonomy with the MITRE ATT&CK framework. The model leverages cmdstanr, Bayesian inference, and Monte Carlo simulation to estimate annualized loss exposure (ALE), incident frequency, and loss exceedance curves in a transparent and reproducible workflow.\nThe framework ingests MITRE ATT&CK data, computes weighted defensive control strengths across relevant tactics, and builds Beta-distributed priors for per-stage attacker success probabilities. Frequency is modeled using a lognormal prior calibrated to expert-defined credible intervals. The model then performs posterior predictive simulation of attacker progression using tactic-level success probabilities, retry dynamics, fallback behavior, and stochastic detection effects, yielding scenario-specific estimates of successful incidents per year. Severity modeling follows FAIR principles, combining lognormal bodies with bounded Pareto tails to represent high-impact legal and reputational loss events.\nAll modeling components are implemented natively in R, including data preparation, Bayesian inference through cmdstanr, and visualization using ggplot2. The result is an end-to-end analytic workflow that unifies threat intelligence, control effectiveness, and financial loss modeling in a transparent, extensible, and reproducible structure suitable for cyber portfolio risk assessment.\nThis presentation will walk through the modeling design, R implementation, diagnostics, and practical applications, and will demonstrate how an ATT&CK-aligned Bayesian FAIR model can support more rigorous, data-driven cyber-risk estimation.\nGitHub Repository: https://github.com/joshua-m-connors/cyber-incident-mcmc-cmdstanr\n\n\nBio\nJosh Connors is an information technology and security risk management professional with more than 15 years of experience helping organizations quantify and manage cyber risk. He has supported technology and security leaders at multiple Fortune 500 enterprises, applying quantitative risk analysis to evaluate and prioritize tens of millions of dollars in strategic security and technology investments.\nHis current work focuses on advancing integrated cyber-risk measurement by unifying diverse security frameworks—most notably FAIR and MITRE ATT&CK—with modern statistical and Bayesian modeling techniques. Josh’s research and development efforts aim to improve the accuracy, transparency, and decision usefulness of cyber-risk analytics for both practitioners and executives.\nLinkedIn Profile: www.linkedin.com/in/joshuaconnors"
  },
  {
    "objectID": "Abstracts.html#defining-granular-risk-groups-a-reproducible-workflow-for-multi-threshold-survival-analysis",
    "href": "Abstracts.html#defining-granular-risk-groups-a-reproducible-workflow-for-multi-threshold-survival-analysis",
    "title": "R!sk 2026 Abstracts",
    "section": "Defining Granular Risk Groups: A Reproducible Workflow for Multi-Threshold Survival Analysis",
    "text": "Defining Granular Risk Groups: A Reproducible Workflow for Multi-Threshold Survival Analysis\nDate & Time: February 19 at 8:00am, Eastern\n\nAbstract\nIn risk modeling, categorising continuous variables—such as biomarker levels or credit scores—is essential for creating distinct risk groups. While existing R tools often optimise a single threshold (creating “High” vs “Low” groups), they lack a systematic framework for identifying multiple cut-points. This limitation forces analysts to rely on simple binary splits, often masking complex biological realities like U-shaped risk profiles.\nIn this lightning talk, I will introduce OptSurvCutR, an R package designed to bridge this gap. I will demonstrate how the package uses a reproducible workflow to systematically scan for optimal thresholds, allowing for the automatic detection of 3, 4, or 5+ distinct risk strata. We will also discuss how the package integrates Maximally Selected Rank Statistics (MSRS) and bootstrap validation to ensure these granular risk models are statistically robust and free from overfitting.\n\n\nBio\nDr. Payton Yau is a Lecturer in Computational Biology at Nottingham Trent University (UK). He holds a PhD in Biosciences and specializes in high-dimensional microbiome analysis and systems biology. He previously conducted extensive research at Scotland’s Rural College (SRUC) and brings industry experience from Intelligent OMICS Ltd, where he applied artificial neural networks to complex clinical datasets. He is the creator of OptSurvCutR, an R package developed to address the reproducibility crisis in survival threshold selection. His current research integrates computational oncology, microbiome interactions, and non-linear risk modeling.\n\n\nSocial Media\n\nLinkedIn: https://www.linkedin.com/in/tungon/\nGitHub: https://github.com/paytonyau\nX/Twitter: https://x.com/PaytonYau"
  },
  {
    "objectID": "Abstracts.html#risky-viz",
    "href": "Abstracts.html#risky-viz",
    "title": "R!sk 2026 Abstracts",
    "section": "Risky viz",
    "text": "Risky viz\nDate & Time: February 19 at 8:15am, Eastern\n\nAbstract\nOverwhelming Red-Amber-Green colour schemes, confused stakeholders unsure where to start in your scorecard, and visuals that you have to keep coming back to all share one common theme — they are not accessible, intuitive or memorable enough for their end-users. This lightening talk will explore dataviz design design principles and their implementation within ggplot, to maximise the impact of our visuals in identifying and communicating risk:\n\nBuilding accessible and meaningful traffic-light colour schemes\n\nAnnotating visualisations with end users in mind\n\nOptimising typography to keep the main thing the main thing\n\nProgrammatically creating narrative titles\n\nParameterising visual alert thresholds\n\nWe’ll build and improve a graph over the course of the talk, to go from something functional to something compelling, exploring the rationale behind each step.\n\n\nBio\nCara is a data visualisation consultant with an academic background, specialising in helping research teams and data-driven organisations turn their data insights into to clear and compelling visualisations.\nAfter her PhD in Psychology and a spell teaching research methods at Edinburgh Uni, she embarked on a career in psychometrics at the Royal college of Surgeons of Edinburgh. After ten years of helping surgeons and other medical professionals understand complex patterns in exam data, she set out as an independent data visualisation consultant, to continue building creative solutions for a range of different organisations.\nShe lives in Edinburgh, Scotland, with her husband and two young daughters. Cara regularly shares coding tips for dataviz online and genuinely enjoys helping others level up their dataviz skills through talks, bespoke toolkits and organisational training."
  },
  {
    "objectID": "Abstracts.html#conformal-inference-calibrating-uncertainty-with-a-three-way-split",
    "href": "Abstracts.html#conformal-inference-calibrating-uncertainty-with-a-three-way-split",
    "title": "R!sk 2026 Abstracts",
    "section": "Conformal Inference & Calibrating Uncertainty with a Three-Way Split",
    "text": "Conformal Inference & Calibrating Uncertainty with a Three-Way Split\nDate & Time: February 19 at 8:30am, Eastern\n\nAbstract\nConformal prediction methods allow us to create reliable prediction intervals for machine learning models. Machine learning models typically create a single point forecast, conformal prediction is a model agnostic method to add prediction intervals to any point forecast. This talk will introduce the three-way data split: one set for training the point forecast, one for interval calibration, and one for testing both. Using this approach, we can compute intervals for regression or classification models that provide valid coverage guarantees. The presentation will include a brief demo using open-source tools in R, illustrating how to implement the method and interpret the results. Attendees will leave with a straightforward workflow to assess prediction confidence in their own projects.\nLightning Talk Breakdown (10-minute speedrun!)\n\nWhy Uncertainty Matters: Brief motivation for prediction intervals.\nWhat is Conformal Prediction: Simple overview and key concepts.\nThree-Way Split Method: How to split data for calibration and why it works.\nExample: Quick code walkthrough showing the method in practice.\n\n\n\nBio\nFrank Hull is a director of data science & analytics, leading a data science team in the energy sector, an open source contributor, and a developer of {kuzco}, an R package that reimagines how image classification and computer vision can be approached using large language models (LLMs). With a passion for building tools that make advanced technology accessible to non-specialists, Frank has also contributed to the R ecosystem through multiple projects, and actively maintains his work on GitHub and his personal site."
  },
  {
    "objectID": "Abstracts.html#lightweight-transfer-learning-for-financial-forecasting-using-quasi-randomized-networks",
    "href": "Abstracts.html#lightweight-transfer-learning-for-financial-forecasting-using-quasi-randomized-networks",
    "title": "R!sk 2026 Abstracts",
    "section": "Lightweight Transfer Learning for Financial Forecasting Using Quasi-Randomized Networks",
    "text": "Lightweight Transfer Learning for Financial Forecasting Using Quasi-Randomized Networks\nDate & Time: February 19 at 8:45am, Eastern\n\nAbstract\nThis work introduces a transfer learning method for probabilistic financial time series forecasting. The model is first trained on synthetic returns exhibiting market stylized facts, using Bayesian-optimized architectural priors. It is then fine-tuned on real data via convex optimization, with prediction intervals generated via moving block bootstrap.\nCode & Data: (could be modified, but the idea is in there): https://github.com/thierrymoudiki/2025-09-05-transfer-learning-ridge2f\n\n\nBio\nSenior Data Scientist, Actuary, and Researcher specializing in machine learning, time series forecasting, and financial modeling. Founder and lead of Techtonique LLC (Delaware, USA), selected for the Microsoft for Startups program. PhD from Université Lyon 1 with expertise bridging academic research, open-source software development, and practical financial applications. Creator of widely-adopted R and Python packages downloaded over 1 million times, with research presented at leading international conferences including the International Symposium on Forecasting and published in peer-reviewed venues."
  },
  {
    "objectID": "Abstracts.html#shiny-apps-for-time-series-analysis-and-extremes-in-hydrology",
    "href": "Abstracts.html#shiny-apps-for-time-series-analysis-and-extremes-in-hydrology",
    "title": "R!sk 2026 Abstracts",
    "section": "Shiny Apps for Time Series Analysis and Extremes in Hydrology",
    "text": "Shiny Apps for Time Series Analysis and Extremes in Hydrology\nDate & Time: February 19 at 9:00am, Eastern\n\nAbstract\nDevelopment of shiny apps to quickly and easily apply different statistical analyses to hydrological time series at different time scales and for different variables. The applications seek to support decision-making associated with risks in hydrology, with platforms that allow data management, adequate visualization, and downloading of all results.\n\n\nBio\nAlonso is a civil engineer with a major in hydraulics, sanitary-environment from the University of Chile, and a master’s degree in hydrology and water management from University of Alcalá (Spain). With 17 years of professional experience in the engineering consultancy, he has developed essential skills in the use of R and Python applied to hydrology, water resources and climate change."
  },
  {
    "objectID": "Abstracts.html#from-forecasts-to-action-a-data-pipeline-for-air-quality-monitoring-and-notifications",
    "href": "Abstracts.html#from-forecasts-to-action-a-data-pipeline-for-air-quality-monitoring-and-notifications",
    "title": "R!sk 2026 Abstracts",
    "section": "From Forecasts to Action: A Data Pipeline for Air Quality Monitoring and Notifications",
    "text": "From Forecasts to Action: A Data Pipeline for Air Quality Monitoring and Notifications\nDate & Time: February 19 at 9:15am, Eastern\n\nAbstract\nPoor air quality has severe impacts on human health. While outdoor air quality is generally acceptable in Western countries, climate-related events such as wildfires and Saharan dust storms can lead to extreme pollution levels. To monitor air quality forecasts and inform individuals about dangerous exposure levels, I developed an automated data pipeline to monitor the risk. A scheduled daily GitHub Action renders a Quarto document and publishes it to Quarto Pub. The document retrieves regional forecast data from the WMO Barcelona Regional Center, obtains the local forecast for the city of Madrid, and sends email notifications (via the Sendgrid API) with recommendations and protective actions if pollution levels will be hazardous.\nBy providing automated email alerts, I aim to mitigate exposure to hazardous air quality. A complementary research project evaluates how people respond to these alerts during extreme pollution events and quantifies the share of outdoor pollution that infiltrates indoor spaces.\n\n\nBio\nBjörn is an environmental and behavioral economist at the University of Hamburg. His research addresses critical societal issues by using state-of-the-art empirical methods and modern technology. His work is concerned with the evaluation of policies to improve air quality, public good contributions, common-pool resource problems, and distributional preferences.\nhttps://www.linkedin.com/in/bjoern-bos/"
  },
  {
    "objectID": "Abstracts.html#dwd-temperature-explorer-an-open-access-r-shiny-dashboard-for-climate-informed-risk-governance-in-germany",
    "href": "Abstracts.html#dwd-temperature-explorer-an-open-access-r-shiny-dashboard-for-climate-informed-risk-governance-in-germany",
    "title": "R!sk 2026 Abstracts",
    "section": "DWD Temperature Explorer: An Open-Access R Shiny Dashboard for Climate-Informed Risk Governance in Germany",
    "text": "DWD Temperature Explorer: An Open-Access R Shiny Dashboard for Climate-Informed Risk Governance in Germany\nDate & Time: February 19 at 9:30am, Eastern\n\nAbstract\nThis talk presents the DWD Temperature Explorer, an open-access R Shiny dashboard designed to facilitate transparent, reproducible, and policy-relevant exploration of temperature data from the German Weather Service (DWD). The application enables both non-technical and technical users to analyze spatial and temporal temperature patterns across Germany, supporting climate-informed risk governance, adaptation planning, and educational use. The presentation discusses how such dashboards can be developed to explore and democratize public data, making climate information more open and accessible to all. Additionally, during the live discussion, an example from the Global South—specifically Ecuador—is presented, highlighting visualization design approaches for meteorological data that are essential for risk assessment and for developing adaptation and mitigation tools in climate-vulnerable contexts.\n\n\nBio\nDr. Cesar Ivan Alvarez is an Ecuadorian geospatial specialist currently working as a research scientist at the University of Augsburg (Germany), at the interface of remote sensing, GeoAI, climate analytics, and environmental risk assessment. His research focuses on transforming open Earth observation and climate data into decision-support tools for climate resilience, public health, and risk governance. He has experience developing open-source dashboards and geospatial applications using R, Python, and Google Earth Engine, with several applications shared openly with the broader community. His work emphasizes transparency, reproducibility, and stakeholder-oriented climate information systems.\nhttps://www.linkedin.com/in/cesar-ivan-alvarez-0847253a/"
  },
  {
    "objectID": "Abstracts.html#mcmodule-an-r-package-for-multi-pathway-monte-carlo-risk-assessment",
    "href": "Abstracts.html#mcmodule-an-r-package-for-multi-pathway-monte-carlo-risk-assessment",
    "title": "R!sk 2026 Abstracts",
    "section": "mcmodule: An R Package for Multi-Pathway Monte Carlo Risk Assessment",
    "text": "mcmodule: An R Package for Multi-Pathway Monte Carlo Risk Assessment\nDate & Time: February 19 at 10:15am, Eastern\n\nAbstract\nmcmodule is an R package for building modular, multi‑pathway quantitative risk assessment models using two‑dimensional Monte Carlo simulation. It extends mc2d to handle complex introduction pathways, multiple scenarios and hierarchical aggregation while keeping uncertainty representation transparent. This talk presents the core design of mcmodule, which organises stochastic nodes into metadata‑rich modules with explicit keys, inputs and model expressions. We also show how to visualise risk model networks and use integrated diagnostics to assess stability, sensitivity and uncertainty. Using a veterinary public health case study, we show how mcmodule manages what‑if scenarios, automates multivariate operations and improves transparency and communication.\n\n\nBio\nNatalia Ciria is a veterinarian and PhD candidate in epidemiology at the Department of Animal Health and Anatomy, Autonomous University of Barcelona (UAB). Her work focuses on applying data science, data visualisation and mathematical modelling to problems in animal and global health. Within the European BIOSECURE project, she develops quantitative risk assessment models to reduce the risk of infectious disease introduction into livestock farms. She is an author and maintainer of several open‑source R tools, including the mcmodule package, the parametra parameter database, and the farmrisk algorithm.\n\nWebsite: https://nataliaciria.com\nLinkedIn: https://www.linkedin.com/in/natalia-ciria-artiga/\nGitHub: https://github.com/NataliaCiria"
  },
  {
    "objectID": "Abstracts.html#mapping-the-unknown-during-an-active-outbreak-lessons-from-geospatial-risk-modelling-of-oropouche-virus",
    "href": "Abstracts.html#mapping-the-unknown-during-an-active-outbreak-lessons-from-geospatial-risk-modelling-of-oropouche-virus",
    "title": "R!sk 2026 Abstracts",
    "section": "Mapping the Unknown During an Active Outbreak: Lessons from Geospatial Risk Modelling of Oropouche Virus",
    "text": "Mapping the Unknown During an Active Outbreak: Lessons from Geospatial Risk Modelling of Oropouche Virus\nDate & Time: February 19 at 11:05am, Eastern\n\nAbstract\nSince late 2023, Latin America has experienced an unprecedented increase in reported Oropouche virus (OROV) infections, with widespread transmission and previously uncommon morbidity and mortality, leading to its classification by the World Health Organization as a high regional public health risk.\nIn the context of limited epidemiological knowledge and heterogeneous surveillance, this talk focuses on the application of a geospatial modelling framework to support outbreak-time risk assessments. We integrated retrospective serological data from over 9,400 samples across six Latin American countries with presence-only incidence data from the outbreak, using MaxEnt models implemented in R to characterise spatial transmission risk.\nOur analysis for example highlighted the role of climatic factors in shaping transmission risk and illustrates how retrospective serological data can inform spatial risk assessment when outbreak surveillance data are sparse.\nMore broadly, the talk discusses practical considerations for geospatial analysis during emerging infectious disease events, including data limitations, uncertainty, and time constraints.\n\n\nBio\nAnna Fruehauf is an MD–PhD candidate in infectious disease epidemiology motivated by integrating quantitative and digital approaches to support equitable public health action and clinical care. With formal training in public health and epidemiology, including from the London School of Hygiene & Tropical Medicine and the University of Cambridge, and field epidemiology experience across African and Latin American countries, working with organizations like Partners In Health and the World Health Organization, she develops reproducible analytical workflows that inform decision-making in data-sparse settings. Drawing on an interdisciplinary background spanning data science, clinical medicine, and global health, her approach is grounded in the conviction that modelling and digital tools should meaningfully inform both public health action and clinical decision-making in settings facing structural disadvantage.\nhttps://www.linkedin.com/in/anna-fr%C3%BChauf-67a9b8123/"
  },
  {
    "objectID": "Abstracts.html#closing-remarks",
    "href": "Abstracts.html#closing-remarks",
    "title": "R!sk 2026 Abstracts",
    "section": "Closing Remarks",
    "text": "Closing Remarks\nDate & Time: February 19 at 11:55am, Eastern\n\nAbstract\nClosing remarks video asking for survey responses and topics for R!sk webinars and 2027 event\n\n\nBio\n30 years of building content strategies to help companies acquire and support customers. Successfully rebranded and created content programs to help build and sell 4 different companies. Built programs to identify and remedy content management issues affecting content performance. Managed outreach programs to open source communities through digital, hybrid, and IRL events.\n@terrychristiani.bsky.social"
  },
  {
    "objectID": "sponsors.html",
    "href": "sponsors.html",
    "title": "Sponsors",
    "section": "",
    "text": "Sponsorship opportunities: Coming Soon.\nFor sponsor inquiries: TBD."
  }
]